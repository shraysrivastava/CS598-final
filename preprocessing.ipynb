{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/Motor_Vehicle_Collisions_-_Crashes.csv\")\n",
    "\n",
    "essential_columns = [\n",
    "    'CRASH DATE', 'CRASH TIME', 'LATITUDE', 'LONGITUDE', 'BOROUGH',\n",
    "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
    "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
    "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED',\n",
    "    'COLLISION_ID'\n",
    "]\n",
    "\n",
    "df = df[essential_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>NUMBER OF PERSONS INJURED</th>\n",
       "      <th>NUMBER OF PERSONS KILLED</th>\n",
       "      <th>NUMBER OF PEDESTRIANS INJURED</th>\n",
       "      <th>NUMBER OF PEDESTRIANS KILLED</th>\n",
       "      <th>NUMBER OF CYCLIST INJURED</th>\n",
       "      <th>NUMBER OF CYCLIST KILLED</th>\n",
       "      <th>NUMBER OF MOTORIST INJURED</th>\n",
       "      <th>NUMBER OF MOTORIST KILLED</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/01/2023</td>\n",
       "      <td>1:29</td>\n",
       "      <td>40.621790</td>\n",
       "      <td>-73.970024</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4675373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>9:35</td>\n",
       "      <td>40.667202</td>\n",
       "      <td>-73.866500</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4456314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12/14/2021</td>\n",
       "      <td>8:13</td>\n",
       "      <td>40.683304</td>\n",
       "      <td>-73.917274</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4486609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/14/2021</td>\n",
       "      <td>8:17</td>\n",
       "      <td>40.868160</td>\n",
       "      <td>-73.831480</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4486660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/14/2021</td>\n",
       "      <td>21:10</td>\n",
       "      <td>40.671720</td>\n",
       "      <td>-73.897100</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4487074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CRASH DATE CRASH TIME   LATITUDE  LONGITUDE   BOROUGH  \\\n",
       "2   11/01/2023       1:29  40.621790 -73.970024  BROOKLYN   \n",
       "9   09/11/2021       9:35  40.667202 -73.866500  BROOKLYN   \n",
       "10  12/14/2021       8:13  40.683304 -73.917274  BROOKLYN   \n",
       "13  12/14/2021       8:17  40.868160 -73.831480     BRONX   \n",
       "14  12/14/2021      21:10  40.671720 -73.897100  BROOKLYN   \n",
       "\n",
       "    NUMBER OF PERSONS INJURED  NUMBER OF PERSONS KILLED  \\\n",
       "2                         1.0                       0.0   \n",
       "9                         0.0                       0.0   \n",
       "10                        0.0                       0.0   \n",
       "13                        2.0                       0.0   \n",
       "14                        0.0                       0.0   \n",
       "\n",
       "    NUMBER OF PEDESTRIANS INJURED  NUMBER OF PEDESTRIANS KILLED  \\\n",
       "2                               0                             0   \n",
       "9                               0                             0   \n",
       "10                              0                             0   \n",
       "13                              0                             0   \n",
       "14                              0                             0   \n",
       "\n",
       "    NUMBER OF CYCLIST INJURED  NUMBER OF CYCLIST KILLED  \\\n",
       "2                           0                         0   \n",
       "9                           0                         0   \n",
       "10                          0                         0   \n",
       "13                          0                         0   \n",
       "14                          0                         0   \n",
       "\n",
       "    NUMBER OF MOTORIST INJURED  NUMBER OF MOTORIST KILLED  COLLISION_ID  \n",
       "2                            1                          0       4675373  \n",
       "9                            0                          0       4456314  \n",
       "10                           0                          0       4486609  \n",
       "13                           2                          0       4486660  \n",
       "14                           0                          0       4487074  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(\n",
    "    df['CRASH DATE'] + ' ' + df['CRASH TIME'],\n",
    "    format='%m/%d/%Y %H:%M'\n",
    ")\n",
    "\n",
    "df['iso_datetime'] = df['datetime'].dt.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['CRASH DATE', 'CRASH TIME', 'datetime']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weather_data(lat, lon, date_time, timezone='America/New_York', retries=3, timeout=5):\n",
    "    \n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    date_str = date_time[:10]  # YYYY-MM-DD\n",
    "    hour_str = date_time[11:]  # HH:MM\n",
    "\n",
    "    params = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'start_date': date_str,\n",
    "        'end_date': date_str,\n",
    "        'hourly': 'temperature_2m,precipitation,weathercode',\n",
    "        'timezone': timezone,\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # extracts weather for the specific hour\n",
    "            target_time = f\"{date_str}T{hour_str}\"\n",
    "            if 'hourly' in data and 'time' in data['hourly']:\n",
    "                times = data['hourly']['time']\n",
    "                if target_time in times:\n",
    "                    index = times.index(target_time)\n",
    "                    return {\n",
    "                        'temperature': data['hourly']['temperature_2m'][index],\n",
    "                        'precipitation': data['hourly']['precipitation'][index],\n",
    "                        'weathercode': data['hourly']['weathercode'][index],\n",
    "                    }\n",
    "            return None\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout occurred for lat: {lat}, lon: {lon}, datetime: {date_time} (attempt {attempt + 1}/{retries})\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP error for lat: {lat}, lon: {lon}, datetime: {date_time}: {e}\")\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error for lat: {lat}, lon: {lon}, datetime: {date_time}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"Skipping lat: {lat}, lon: {lon}, datetime: {date_time} after {retries} attempts.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weathercode_to_description(weathercode):\n",
    "    if weathercode in [0, 1]:\n",
    "        return 'Clear'\n",
    "    elif weathercode in [2, 3]:\n",
    "        return 'Cloudy'\n",
    "    elif weathercode in [45, 48]:\n",
    "        return 'Fog/Haze'\n",
    "    elif weathercode in [51, 53, 55, 56, 57]:\n",
    "        return 'Drizzle'\n",
    "    elif weathercode in [61, 80]:\n",
    "        return 'Light Rain'\n",
    "    elif weathercode in [63, 65, 66, 67, 81, 82]:\n",
    "        return 'Heavy Rain'\n",
    "    elif weathercode in [71, 85]:\n",
    "        return 'Light Snow'\n",
    "    elif weathercode in [73, 75, 77, 86]:\n",
    "        return 'Heavy Snow'\n",
    "    elif weathercode in [95, 96, 99]:\n",
    "        return 'Thunderstorm'\n",
    "    else:\n",
    "        return 'Unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yearly_sample(\n",
    "    base_df,\n",
    "    year,\n",
    "    non_severe_frac=0.10,\n",
    "    max_per_month=1000,\n",
    "    max_per_borough=2000,\n",
    "    random_state=42\n",
    "):\n",
    "    year_df = base_df[base_df['year'] == year].copy()\n",
    "    if year_df.empty:\n",
    "        print(f\"No records for year {year}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    severe_mask = (year_df['NUMBER OF PERSONS INJURED'] > 0) | (year_df['NUMBER OF PERSONS KILLED'] > 0)\n",
    "    severe_accidents = year_df[severe_mask]\n",
    "    non_severe = year_df[~severe_mask]\n",
    "\n",
    "    if not non_severe.empty:\n",
    "        non_severe_sample = non_severe.sample(\n",
    "            frac=min(non_severe_frac, 1.0),\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        non_severe_sample = pd.DataFrame(columns=year_df.columns)\n",
    "\n",
    "    sampled_df = pd.concat([severe_accidents, non_severe_sample])\n",
    "\n",
    "    sampled_df = (\n",
    "        sampled_df\n",
    "        .groupby('month', group_keys=False)\n",
    "        .apply(lambda x: x.sample(min(len(x), max_per_month), random_state=random_state))\n",
    "    )\n",
    "\n",
    "    sampled_df = (\n",
    "        sampled_df\n",
    "        .groupby('BOROUGH', group_keys=False)\n",
    "        .apply(lambda x: x.sample(min(len(x), max_per_borough), random_state=random_state))\n",
    "    )\n",
    "\n",
    "    sampled_df = sampled_df.drop_duplicates(subset=['LATITUDE', 'LONGITUDE', 'datetime'])\n",
    "\n",
    "    print(f\"Year {year}: sampled {sampled_df.shape[0]} rows.\")\n",
    "    return sampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_weather(sample_df):\n",
    "    if sample_df.empty:\n",
    "        return sample_df\n",
    "\n",
    "    sample_df = sample_df.copy()\n",
    "    sample_df['temperature'] = np.nan\n",
    "    sample_df['precipitation'] = np.nan\n",
    "    sample_df['weather_condition'] = np.nan\n",
    "\n",
    "    for index, row in tqdm(sample_df.iterrows(), total=sample_df.shape[0]):\n",
    "        weather_data = get_weather_data(row['LATITUDE'], row['LONGITUDE'], row['iso_datetime'])\n",
    "        if weather_data:\n",
    "            sample_df.at[index, 'temperature'] = weather_data['temperature']\n",
    "            sample_df.at[index, 'precipitation'] = weather_data['precipitation']\n",
    "            sample_df.at[index, 'weather_condition'] = map_weathercode_to_description(weather_data['weathercode'])\n",
    "\n",
    "    sample_df.dropna(subset=['temperature', 'precipitation', 'weather_condition'], inplace=True)\n",
    "\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_specific_years(\n",
    "    base_df,\n",
    "    years=range(2016, 2026),\n",
    "    non_severe_frac=0.10,\n",
    "    max_per_month=1000,\n",
    "    max_per_borough=2000,\n",
    "    random_state=42\n",
    "):\n",
    "\n",
    "    year_to_df = {}\n",
    "\n",
    "    for y in years:\n",
    "        if y not in base_df[\"year\"].unique():\n",
    "            print(f\"Year {y} not found in dataset, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n Processing year: {y}\")\n",
    "        sampled_y = build_yearly_sample(\n",
    "            base_df,\n",
    "            year=y,\n",
    "            non_severe_frac=non_severe_frac,\n",
    "            max_per_month=max_per_month,\n",
    "            max_per_borough=max_per_borough,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        if sampled_y.empty:\n",
    "            print(f\" No rows sampled for year {y}.\")\n",
    "            continue\n",
    "\n",
    "        year_to_df[y] = sampled_y\n",
    "\n",
    "    return year_to_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_samples = sample_specific_years(df)\n",
    "\n",
    "sample_2016 = yearly_samples.get(2016)\n",
    "sample_2017 = yearly_samples.get(2017)\n",
    "sample_2018 = yearly_samples.get(2018)\n",
    "sample_2019 = yearly_samples.get(2019)\n",
    "sample_2020 = yearly_samples.get(2020)\n",
    "sample_2021 = yearly_samples.get(2021)\n",
    "sample_2022 = yearly_samples.get(2022)\n",
    "sample_2023 = yearly_samples.get(2023)\n",
    "sample_2024 = yearly_samples.get(2024)\n",
    "sample_2025 = yearly_samples.get(2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_year(base_df, year, output_dir=\"datasets\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Sampling {year}...\")\n",
    "    sampled_df = build_yearly_sample(base_df, year)\n",
    "\n",
    "    print(f\"Enriching {year}...\")\n",
    "    enriched_df = enrich_with_weather(sampled_df)\n",
    "\n",
    "    output_path = f\"{output_dir}/enriched_{year}.csv\"\n",
    "    enriched_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Saved enriched dataset for {year} to {output_path}\")\n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cells below should be run only once every 12 hours to avoid hitting API limits. Expect a long runtime as there are several rows to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_single_year(df, 2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
